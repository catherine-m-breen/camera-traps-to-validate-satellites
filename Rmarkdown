---
title: "cameraSatellite_analysis"
author: "Catherine Breen"
date: "1/13/2022"
output:
  html_document: default
  pdf_document: default
---

This notebook is to investigate the following research questions:

How can we use camera traps to supplement MODIS Snow Products on cloudy days and in forests? 

1. How well do camera traps and MOD10A1 values agree? Does vegetation influence agreement?
2. How well do camera traps and MOD10A1F (on cloudy days) agree? Do the number of consecutive cloudy days influence agreement?
3. If we increase the spatial resolution, does that increase agreement? 

If vegetation and clouds decrease agreement, camera traps could supplement by improving ground information to inform snow analysis for ecologists. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load necessary libraries

Plotting: grid, ggplot2, gridExtra
Statistics: pROC, lme4, MuMIn, mgcv, caret 
Data cleaning: abind, caret??

```{r, include=FALSE}
library(gtable)
library(grid)
library(gridExtra)
library(abind)
library(ggplot2)
library(mgcv)
library(caret)
library(tidyr)
library(dplyr)
library(lubridate)

### for models
library(DHARMa)
library(glmmTMB)

# for ROC analysis
library(pROC)


```

## Load data and include cameraID 
(the norwegian word is LokalitetID) as a factor. It will be used as a random effect later on. 

The data was cleaned first in python to remove duplicate photos. The final dataframe includes variables that affect the sensor (NDVI, treecanopy) and
ground conditions (saturation, e.g. day or night).


## rematch MOD10A1F with the data_cgf data just in case 

colnames are 
 [1] "dateFormatted"     "LokalitetID"       "File"              "Date"              "Time"              "SnowCover"        
 [7] "NDVImodis"         "treecanopycover"   "Latitude"          "Longitude"         "mean"              "labels"           
[13] "binary.labels"     "Cloud_Persistence" "CGF_NDSI" 


# MOD10A1 + camera data 
#### cleaned in python

```{r}

data_csv <- read.csv('~/Documents/RemoteSensingPaper/datasets/data_wCovariates_October12-2021.csv')
data <- data_csv[,c("File", "LokalitetID","Date", "Time", "SnowCover","NDSImodis", "NDVImodis","treecanopycover", "Latitude","Longitude")]
data$LokalitetID <- factor(data$LokalitetID)
saturation <- read.csv('~/Documents/RemoteSensingPaper/datasets/saturation_sort_results.csv')
data <- merge(data, saturation, by.x = 'File', by.y = 'filename', all.x = TRUE)
print(cor(data$NDSImodis, data$SnowCover)) ##0.81

data$dateFormatted <- as.POSIXct(as.Date(data$Date,format='%m/%d/%y'))
rm(data_csv,  saturation)

####################### UPDATE #####################
# Narrow study period to November to April 
# After inspecting the spatial autocorrelation, we found that fitting for spatial autocorrelation, did not improve residuals
# Instead it seems like the October values are throwing it off because it is inflating our 0 values?? I am not really sure how to explain it. 
# 
# data2018 <- data %>% filter(dateFormatted < ('2018-03-30')) %>% filter(dateFormatted > ('2017-11-01'))
# data2019 <- data %>% filter(dateFormatted < ('2019-03-30')) %>% filter(dateFormatted > ('2018-11-01'))
# data2020 <- data %>% filter(dateFormatted < ('2020-03-30')) %>% filter(dateFormatted > ('2019-11-01'))
# data <- rbind(data2018,data2019,data2020)

####################################################

print(length(unique(data$LokalitetID))) ##665 cameras

```

##coordinates table

```{r}
coordinates <- read.csv('/Users/catherinebreen/Documents/Chapter 3/data/ScandCam_AllCameras_05072020_GEE.csv')
coordinates <- coordinates[,c('LokalitetID', 'Latitude', 'Longitude')]

coordinates[coordinates$LokalitetID == 671,]
coordinates[coordinates$LokalitetID == 894,]

```


# MOD10A1F + camera data 
#### cleaned in python

```{r}

### random subset ###
data_cgf_csv <- read.csv("~/Documents/RemoteSensingPaper/datasets/cgfdata_wCovariates_October12-2021_30scale.csv")
data_cgf_csv <- data_cgf_csv[,c("File", "LokalitetID","Date", "Time", "SnowCover")]
data_cgf_csv <- distinct(data_cgf_csv, LokalitetID,Date, .keep_all= TRUE)
#cor(df$SnowCover, df$mean) ## 0.81
data_cgf_csv$Date <- as.POSIXct(data_cgf_csv$Date) ##
data_cgf_csv <- data_cgf_csv[,c("File", "LokalitetID","Date", "Time", "SnowCover")]
### tanvi images ### would need to do a kappa statistic 

length(unique(data_cgf_csv$Date))
hist(data_cgf_csv$Date, breaks = 376)
length(unique(data$Date))


########################## additional images
# tanvi_images <- read.csv('/Users/catherinebreen/Documents/RemoteSensingPaper/datasets/tanvi_labeled_images_cgf.csv')
# tanvi_images <- tanvi_images[tanvi_images$PoorQualilty == FALSE,]
# tanvi_images <- tanvi_images[,c("File", "Date", "Time", "SnowCover")]
# tanvi_images$LokalitetID <- sapply(strsplit(tanvi_images$File,"_"), getElement, 1)
# tanvi_images$Date <- dmy(tanvi_images$Date)
# 
# 
# combined <- rbind(tanvi_images, data_cgf_csv)
# combined <- distinct(combined, File, .keep_all = TRUE)
# combined <- distinct(combined, LokalitetID, Date, .keep_all = TRUE)
# 
# rm(tanvi_images)
###############################

combined <- data_cgf_csv

print(length(unique(combined$LokalitetID))) ##775

################### need this #############################
MOD10A1F_CP <- read.csv('~/Documents/RemoteSensingPaper/datasets/MOD101AF_CPmosaic_projSRORG.csv')
MOD10A1F_CP$dateFormatted <- sapply(strsplit(MOD10A1F_CP$imageId,"T"), getElement, 1)
MOD10A1F_CP$dateFormatted<- as.POSIXct(MOD10A1F_CP$dateFormatted)

MOD10A1F_CGF <- read.csv('~/Documents/RemoteSensingPaper/datasets/MOD101AF_CGFmosaic_projSRORG.csv') #read.csv('/Users/catherinebreen/Downloads/CGFtest_date_30scale.csv')#
MOD10A1F_CGF$dateFormatted <- as.POSIXct(sapply(strsplit(MOD10A1F_CGF$imageId,"T"), getElement, 1))
MOD10A1F <- merge(MOD10A1F_CP, MOD10A1F_CGF, by.x = c("LokalitetID", "dateFormatted"), by.y = c("LokalitetID", "dateFormatted"))
MOD10A1F <- MOD10A1F[,-c(3:4,6:8,10)]
colnames(MOD10A1F) <- c('LokalitetID', 'dateFormatted', 'Cloud_Persistence', 'CGF_NDSI')
MOD10A1F <- dplyr::filter(MOD10A1F, (dateFormatted >= as.POSIXct('2017-10-01') & dateFormatted <= as.POSIXct('2018-04-01')) |
                            dateFormatted >= as.POSIXct('2018-11-01') & dateFormatted <= as.POSIXct('2019-04-01') |
                            dateFormatted >= as.POSIXct('2019-11-01') & dateFormatted <= as.POSIXct('2020-04-01')) ##
MOD10A1F <- na.omit(MOD10A1F) ## won't do anything
rm(MOD10A1F_CGF, MOD10A1F_CP)


#CGFdata <- merge(data_cgf_csv, MOD10A1F, by.x= c("Date", "LokalitetID"), by.y = c("dateFormatted", "LokalitetID"))

CGFdata <- merge(combined, MOD10A1F, by.x= c("Date", "LokalitetID"), by.y = c("dateFormatted", "LokalitetID"))
print(nrow(CGFdata)) #10374
colnames(CGFdata)
CGFdata <- CGFdata[!is.na(CGFdata$CGF_NDSI),] ## none are found
CGFdata <- CGFdata[!duplicated(CGFdata$File), ]
CGFdata <- CGFdata[CGFdata$CGF_NDSI <= 100,] ## 10,171
CGFdata <- na.omit(CGFdata)
cor(CGFdata$CGF_NDSI, CGFdata$SnowCover) ### 0.6735176 ## 0.69 ## 0.81
print(nrow(CGFdata)) ## 10171

rm(data_cgf_csv, data_csv, combined, saturation)

hist(MOD10A1F$Cloud_Persistence, n = 50)
hist(CGFdata$Cloud_Persistence, n = 20)
#test <- CGFdata[CGFdata$Cloud_Persistence > 21,]
#hist(test$Cloud_Persistence, n=14)


```

#### Data summary
## Alternative way to do histograms put columns side by side

```{r}
## transform today into long format


hist_function <- function(data) {
  ## transform today into long format
  histogramData <- data[, c("LokalitetID", "SnowCover", "NDSImodis")]
  colnames(histogramData) <-
    c('LokalitetID', 'camera', 'MODIS\nsatellite')
  
  histogramData$camera <-
    replace(histogramData$camera, histogramData$camera == 1 , 25)
  histogramData$camera <-
    replace(histogramData$camera, histogramData$camera == 2 , 50)
  histogramData$camera <-
    replace(histogramData$camera, histogramData$camera == 3 , 75)
  histogramData$camera <-
    replace(histogramData$camera, histogramData$camera == 4 , 100)
  
  histogramDataLong <-
    gather(histogramData, instrument, measurement, 2:3, factor_key = TRUE)
  
  breaks.major <- c(0, 15, 37.5, 52.5, 67.5, 82.5, 95, 100)
  breaks.minor <- c(0, 25, 50, 75, 100)
  labels.minor <- c("~0%", "~25%", "~50%", "~75%", "~100%")
  
  histograms <-
    ggplot(histogramDataLong,
           aes(x = measurement, color = instrument, fill = instrument)) + geom_histogram(
             position = "dodge",
             alpha = 0.3,
             col = "black",
             bins = 5
           ) +
    theme(
      panel.background = element_blank(),
      text = element_text(size = 10),
      axis.line = element_line(colour = "black"),
      legend.position = c(.5, .85),
    ) + scale_x_continuous(name = 'snow value',
                           labels = labels.minor,
                           breaks = breaks.minor) +
    scale_fill_manual(name = "instrument", values = c("#C7C9CB", "black"))  #+ geom_vline(xintercept=13, linetype="solid", color = "white", size=2)+
  
  return(histograms)
}

hist_function(data)

#+
#geom_vline(xintercept=20, linetype="solid", color = "white", size=2)+
#etc

```
## REVISION ADDITION
before general correlation, check for spatial autocorrelation in data

```{r}
library(spdep)
library(dplyr)
# https://datascienceplus.com/spatial-regression-in-r-part-1-spamm-vs-glmmtmb/


dat_katie <- data
#dat_katie <- dataA1_non_null 
stations_katie <- data.frame(unique(dat_katie$LokalitetID))
names(stations_katie)[names(stations_katie)=="unique.dat_katie.LokalitetID."] <- "LokalitetID"
  
gis_katie <- data.frame(unique(dat_katie[c("LokalitetID", "Latitude", "Longitude")]))
gis_katie <- as.matrix(gis_katie[,2:3])
  
nb_katie <- dnearneigh(gis_katie,0,0.1) # 0.01 dec. deg. = 1.11 km
  
dat_katie <- dat_katie %>%
    group_by(LokalitetID) %>%
    mutate(avg.NDSI = mean(NDSImodis))
moran <- dat_katie %>%
  dplyr::select(LokalitetID, avg.NDSI)
moran <- unique(moran)
moran <- left_join(stations_katie, moran)
moran <- moran %>%
  arrange(LokalitetID)
NDSI_cor_moran_pvalue <- moran.test(moran$avg.NDSI,
             nb2listw(nb_katie, style="W",
                      zero.policy = TRUE),
             zero.policy = TRUE,
             na.action = na.omit) 

#NDSI_cor_moran_pvalue <- 0.471 # not significant

##########################################################
##########################################################

```
```{r}
CGFdata1 <- left_join(CGFdata, coordinates)
#CGFdata1 <- merge(CGFdata, coordinates, by ='LokalitetID')
dat_katie <- CGFdata1
#dat_katie <- dataA1_non_null 
stations_katie <- data.frame(unique(dat_katie$LokalitetID))
names(stations_katie)[names(stations_katie)=="unique.dat_katie.LokalitetID."] <- "LokalitetID"
  
gis_katie <- data.frame(unique(dat_katie[c("LokalitetID", "Latitude", "Longitude")]))
gis_katie <- as.matrix(gis_katie[,2:3])
  
nb_katie <- dnearneigh(gis_katie,0,0.2) # 0.01 dec. deg. = 1.11 km
  
dat_katie <- dat_katie %>%
    group_by(LokalitetID) %>%
    mutate(avg.NDSI = mean(CGF_NDSI))


dat_katie <- dat_katie %>%
    group_by(LokalitetID) %>%
    mutate(avg.NDSI = mean(CGF_NDSI))

moran <- dat_katie %>%
  dplyr::select(LokalitetID, avg.NDSI)
moran <- unique(moran)
moran <- left_join(stations_katie, moran)
moran <- moran %>%
  arrange(LokalitetID)
moran.test(moran$avg.NDSI,
             nb2listw(nb_katie, style="W",
                      zero.policy = TRUE),
             zero.policy = TRUE,
             na.action = na.omit) 

NDSI_cor_moran_pvalue <- 0.1678 # WILL NEED TO ACCOUNT FOR SPATIAL AUTOCORRELATION


```

## overall agreement and distribution

```{r}

A1model <- lm(NDSImodis ~ SnowCover + I(SnowCover^2), data=data)  # build linear regression model on full data
print(A1model)
summary(A1model)
A1_R2 <-  0.6959 

A1Fmodel <- lm(CGF_NDSI ~ SnowCover + I(SnowCover^2), data=CGFdata)  # build linear regression model on full data
print(A1Fmodel)
summary(A1Fmodel)
A1F_R2 <-  0.6845 ### this seems too low

```

## because there is spatial autocorrelation, we should account for it. 
What happens if you account for it, but it isn't there? 
Why would it be in the CGfdataset, but not the NDSI? likely because cloud cover has spatial patterns? 

```{r}

# first we need to create a numeric factor recording the coordinates of the sampled locations

# dat <- CGFdata1
# dat$camCoords <- numFactor(scale(dat$Longitude), scale(dat$Latitude))
# # then create a dummy group factor to be used as a random term
# dat$ID <- factor(rep(1, nrow(dat)))
# 
# #Model using site location
## same as with lme4, but now we are using glmmTMB
# do we have to drop the fixed effect of cameras? It looks like you can, because you now account for grouping 
# by coordinates, not by camera. 
## mat stands for matern, which is a covariance function
# can read about all the covariance structures here: https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html 
# mod_CGF <- glmmTMB(CGF_NDSI ~ SnowCover + I(SnowCover^2) + mat(camCoords + 0 | ID), data = dat, family = gaussian())
# summary(mod_CGF)
# sims_mod_CGF <- simulateResiduals(mod_CGF)
# plot(sims_mod_CGF)

```

### distribution and line of best fit

```{r}

# visualise predictions from best model
# first create new data frames with explanatory values we wish to predict to

boxplot_comparison <- function(data, linear_model, NDSImodis) {
  newdat <-
    data.frame(SnowCover = seq(min(data$SnowCover), max(data$SnowCover), by = 0.1),
               NDSI = 50)
  
  # then predict from best model to new dataframe
  prediction2 <- predict(linear_model, newdat, se.fit = T)
  newdat <- newdat %>%
    mutate(prediction = prediction2[[1]],  se = prediction2[[2]]) %>%
    mutate(lower_ci = prediction - se * 1.96,
           upper_ci = prediction + se * 1.96)
  ### line of best fit on boxplot
  m <-
    ggplot(data, aes(
      x = as.factor(SnowCover),
      y = NDSImodis,
      fill = (SnowCover)
    )) + geom_boxplot(size = .75) +
    geom_line(
      data = newdat,
      aes(SnowCover + 1, prediction),
      color = 'red',
      size = 1.5
    ) + theme(
      text = element_text(size = 22),
      panel.background = element_blank(),
      axis.line = element_line(colour = "grey"),
      axis.text.x = element_text(
        angle = 0,
        hjust = 0.5,
        vjust = 1
      ),
      legend.position = "none", axis.text = element_text(size = 20)
    ) + 
    xlab("Image Labels") + ylab("NDSI Snow Cover") + scale_x_discrete(labels = c('~0%', "~25%", "~50%", "~75%", "~100%")) +
    scale_fill_gradient(low = "gray28", high = "light grey") + labs(tag =
                                                                      "A")
  return(m)
}

boxplot_A1camera <- boxplot_comparison(data, A1model, data$NDSImodis)


```


### agreement 

```{r}

SnowCategoriesA1 <- data$SnowCover
SnowCategoriesA1 <- replace(SnowCategoriesA1, data$SnowCover==1 , 25)
SnowCategoriesA1 <- replace(SnowCategoriesA1, data$SnowCover==2 , 50)
SnowCategoriesA1 <- replace(SnowCategoriesA1, data$SnowCover==3 , 75)
SnowCategoriesA1 <- replace(SnowCategoriesA1, data$SnowCover==4 , 100)

data['agreement'] <- 100 - abs(SnowCategoriesA1 - (data[,'NDSImodis']))

n <- length(data$agreement)
xbar <- mean(data$agreement)
s <- sd(data$agreement)
margin <- qt(0.95,df=n-1)*s/sqrt(n)
lowerinterval <- xbar - margin
upperinterval <- xbar + margin


mean_CI_function <- function(data) {
  #Calculate the mean of the sample data
  mean_value <- mean(data$agreement)
  # Compute the size
  n <- length(data$agreement)
  
  # Find the standard deviation
  standard_deviation <- sd(data$agreement)
  
  # Find the standard error
  standard_error <- standard_deviation / sqrt(n)
  alpha = 0.05
  degrees_of_freedom = n - 1
  t_score = qt(p = alpha / 2,
               df = degrees_of_freedom,
               lower.tail = F)
  margin_error <- t_score * standard_error
  
  # Calculating lower bound and upper bound
  lower_bound <- mean_value - margin_error
  upper_bound <- mean_value + margin_error
  
  # Print the confidence interval
  print(c(mean_value, lower_bound, upper_bound))
}

mean_CI_function(data)
mean_CI_function(data[data$SnowCover == 0,])
mean_CI_function(data[data$SnowCover == 4,])
mean_CI_function(data[data$SnowCover >= 1,])


```

### agreement boxplot

```{r}

H1_analysis <- function(data) {
  H1_figure <-
    ggplot(data = data, aes(
      x = factor(SnowCover),
      y = agreement,
      fill = factor(SnowCover)
    )) +
    geom_boxplot(size = .75) +
    theme(
      text = element_text(size = 14),
      panel.background = element_blank(),
      axis.line = element_line(colour = "grey"),
      axis.text.x = element_text(
        angle = 0,
        hjust = 0.5,
        vjust = 1
      ),
      legend.position = "none"
    ) +
    scale_fill_manual(values = c("#333333", "#666666", "#999999", "#CCCCCC", "#EEEEEE")) +
    xlab("Image Labels") + ylab("Agreement") + scale_x_discrete(labels = c('~0%', "~25%", "~50%", "~75%", "~100%")) +
    labs(tag = "B")
  
  H1_model <- lm(agreement ~ poly(SnowCover, degree = 2), data = data)
  
  newdat_H1 <-
    data.frame(SnowCover = seq(min(data$SnowCover), max(data$SnowCover), by = 0.1),
               agreement = 50)
  
  # then predict from best model to new dataframe
  prediction_H1 <- predict(H1_model, newdat_H1, se.fit = T)
  newdat_H1 <- newdat_H1 %>%
    mutate(prediction = prediction_H1[[1]],  se = prediction_H1[[2]]) %>%
    mutate(lower_ci = prediction - se * 1.96,
           upper_ci = prediction + se * 1.96)
  
  H1agreement_lineOfBestFit <-
    ggplot(data, aes(
      x = as.factor(SnowCover),
      y = agreement,
      fill = (SnowCover)
    )) + geom_boxplot(size = .75) +
    geom_line(
      data = newdat_H1,
      aes(SnowCover + 1, prediction),
      color = 'red',
      size = 1.5
    ) + theme(
      text = element_text(size = 22),
      panel.background = element_blank(),
      axis.line = element_line(colour = "grey"),
      axis.text.x = element_text(
        angle = 0,
        hjust = 0.5,
        vjust = 1
      ),
      legend.position = "none",  axis.text = element_text(size = 20)
    ) +
    xlab("Image Labels") + ylab("Agreement") + scale_x_discrete(labels = c('~0%', "~25%", "~50%", "~75%", "~100%")) +
    scale_fill_gradient(low = "gray28", high = "light grey") + labs(tag =
                                                                      "B")

  
  return(H1agreement_lineOfBestFit)
}

boxplot_agreementcamera <- H1_analysis(data)

grid.arrange(boxplot_A1camera, boxplot_agreementcamera, ncol=2)

```
### reviewer #2 suggestion

```{r}

```

### test for correlation
```{r}
cor.test(data$Latitude, data$treecanopycover)
cor.test(data$treecanopycover, data$NDVImodis)# add NDVI
cor.test(data$Latitude, data$NDVImodis)
cor.test(data$SnowCover, data$Latitude)
cor.test(data$SnowCover, data$NDVImodis) ## these two are correlated
cor.test(data$SnowCover, data$treecanopycover)
cor.test(data$binary.labels, data$treecanopycover)
cor.test(data$binary.labels, data$treecanopycover, method = c("kendall"))
cor.test(data$binary.labels, data$NDVImodis, method = c("kendall"))
cor.test(data$binary.labels, data$Latitude, method = c("kendall"))

```


## linear model 

```{r}

dataA1_non_null <- na.omit(data)
#test <- na.omit(data_Saturation)
dataA1_non_null$LokalitetID <- factor(dataA1_non_null$LokalitetID)
dataA1_non_null$binary.labels <- factor(dataA1_non_null$binary.labels)


##get pvalues
global_model <- lmerTest::lmer(abs(agreement) ~
                          (1|LokalitetID) +
                          scale(Latitude) + 
                         scale(NDVImodis) + scale(treecanopycover) + binary.labels, data = dataA1_non_null) ### what family should I 

summary(global_model)
anova(global_model, test = "F")

library(MuMIn)
r.squaredGLMM(global_model)#, null

# 
# L <- matrix(0, ncol =n, nrow=4)
# L[1,2] <- L[2,3] <- L[3,4] <- L[4,5] <- 1
# ncol(L)
# calcSatterth(global_model, L)
# lmerTest::contestMD(global_model, L) ##
# 
# L <- rbind(c(0, 1, 0), c(0, 0, 1))

```

## reviewer addition: using the variance inflation factor test

Interpretation:
Variance inflation factors range from 1 upwards. The numerical value for VIF tells you (in decimal form) what percentage the variance (i.e. the standard error squared) is inflated for each coefficient. For example, a VIF of 1.9 tells you that the variance of a particular coefficient is 90% bigger than what you would expect if there was no multicollinearity â€” if there was no correlation with other predictors.
A rule of thumb for interpreting the variance inflation factor:
1 = not correlated.
Between 1 and 5 = moderately correlated.
Greater than 5 = highly correlated.


```{r}
library(car)
vif(global_model)
```


## check model residuals

For the dispersion test: 
H0: data is normally distributed
HA: data is not normally distributed 


```{r}

global_model_sims <- simulateResiduals(global_model)
plot(global_model_sims)
summary(testZeroInflation(global_model_sims))

testDispersion(global_model_sims)
## dispersion = 0.97669, p-value = 0.2 


## outlier check
testOutliers(global_model_sims, type = "binomial")
outlier_indices <- outliers(global_model_sims, lowerQuantile =  0, upperQuantile = 1,
  return = c("index", "logical"))
##  could also make lower and upper Q 0 and 1, respectively
outlierdf <- dataA1_non_null[outlier_indices,]
plot(outlierdf$SnowCover, outlierdf$NDSImodis)
plot(outlierdf$Longitude, outlierdf$Latitude)
hist(outlierdf$Latitude, breaks = 200)

plotResiduals(global_model, form = dataA1_non_null$treecanopycover)
#residuals(global_model_sims)
# n <- length(fixef(global_model)) ## how long the contrast vector needs to be

outlierdf[outlierdf['File'] == '2_20200202 (44).JPG',] ## maybe low snow depth
outlierdf[outlierdf['File'] == '43_0-20190531 (2959).JPG',] ## maybe low snow depth
outlierdf[outlierdf['File'] == '252_20190430 (5889).JPG',]


```

### is it sloope?? 

```{r}

slope <- read.csv('/Users/catherinebreen/Documents/Chapter 3/data/slope.csv')
slope$LokalitetID <- factor(slope$LokalitetID)
outlierdf_slope <- left_join(outlierdf, slope)

hist(outlierdf_slope$mean, breaks=10)

```



```{r}
# global_model_slope <- lmerTest::lmer(abs(agreement) ~
#                           (1|LokalitetID) +
#                           scale(Latitude) + 
#                          scale(NDVImodis) + scale(treecanopycover) + scale(mean) + binary.labels, data = data_wslope) ### what family should I
# summary(global_model_slope)
# 
# global_model_sims_SLOPE <- simulateResiduals(global_model_slope)
# plot(global_model_sims_SLOPE)
# 
# vif(global_model_slope)

```

### is it low snow depths ##

```{r}

#snow_depthERA <- read.csv('/Users/catherinebreen/Documents/Chapter 3/data/ERAresults_snowDepth_2017-20.csv')
snow_depthSenorge <- read.csv('/Users/catherinebreen/Documents/Chapter 3/data/results_snowDepth.csv')
snow_depthSenorge$LokalitetID <- as.factor(snow_depthSenorge$LokalitetID)
snow_depthSenorge$date <- sapply(strsplit(snow_depthSenorge$imageId,"T"), getElement, 1)
snow_depthSenorge <- snow_depthSenorge[,c('LokalitetID', 'mean', 'date')]
colnames(snow_depthSenorge) <- c('LokalitetID', 'snowdepth', 'date')
snow_depthSenorge$dateFormatted<- as.POSIXct(snow_depthSenorge$date)
#data$dateFormtted <- dmy(data$Date)
data$dateFormatted <- as.POSIXct(data$Date, format = '%m/%d/%y')
outlierdf$dateFormatted <-as.POSIXct(outlierdf$Date,format = '%m/%d/%y')
#format(snow_depthSenorge$dateFormatted, "%m/%e/%y")   

outlierdf_snow <- merge(outlierdf, snow_depthSenorge, by = c('LokalitetID', 'dateFormatted'))
hist(outlierdf_snow$snowdepth, breaks= 800)
## it's a lot of low snow depth values! 

outlierdf_snow$month <- as.integer(sapply(strsplit(outlierdf_snow$Date,"/"), getElement, 1))
hist(outlierdf_snow$month, breaks = 12)

```
what happens if we incorporate snow depth as a variable 

```{r}
data_wsnow <- merge(data, snow_depthSenorge, by = c('LokalitetID', 'dateFormatted'))
dataA1_non_null

global_model_snow <- lmerTest::lmer(abs(agreement) ~
                          (1|LokalitetID) +
                          scale(Latitude) + scale(treecanopycover) + scale(snowdepth) + scale(snowdepth) + binary.labels, data = data_wsnow) ### what family should I
summary(global_model_snow)

global_model_sims_SNOW <- simulateResiduals(global_model_snow)
plot(global_model_sims_SNOW)

#vif(global_model_slope)

plot(data_wsnow$snowdepth, data_wsnow$agreement)
ggplot(data=data_wsnow, aes(x=snowdepth,y= agreement))+geom_point()+geom_smooth()

data_wsnow$month <- sapply(strsplit(data_wsnow$Date,"/"), getElement, 1)


```

######################################################
Spatial autocorrelation


After running Moran's test in (6.sp_ac.R), we determined that we do need to take into account spatial autocorrelation. 
We will use the glmmTmb package in order to fit model while taking into account that cameras closer to each other will be more similiar than cameras further away. 

https://cran.r-project.org/web/packages/glmmTMB/glmmTMB.pdf 

comparing original model to model with spatial information, as done here: https://stats.stackexchange.com/questions/501396/glmmtmb-random-intercepts-vs-spatial-random-field 

so far.. .
Warning: NA/NaN function evaluation
Warning: NA/NaN function evaluation
Warning: NA/NaN function evaluation
Warning: Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')Warning: Model convergence problem; false convergence (8). See vignette('troubleshooting')


```{r}

library(spdep)
library(dplyr)
# https://datascienceplus.com/spatial-regression-in-r-part-1-spamm-vs-glmmtmb/
dat_katie <- data
#dat_katie <- dataA1_non_null 
stations_katie <- data.frame(unique(dat_katie$LokalitetID))
names(stations_katie)[names(stations_katie)=="unique.dat_katie.LokalitetID."] <- "LokalitetID"

gis_katie <- data.frame(unique(dat_katie[c("LokalitetID", "Latitude", "Longitude")]))
gis_katie <- as.matrix(gis_katie[,2:3])

nb_katie <- dnearneigh(gis_katie,0,0.1) # 0.01 dec. deg. = 1.11 km

dat_katie <- dat_katie %>%
  group_by(LokalitetID) %>%
  mutate(avg.agree = mean(agreement))
moran <- dat_katie %>%
  dplyr::select(LokalitetID, avg.agree)
moran <- unique(moran)
moran <- left_join(stations_katie, moran)
moran <- moran %>%
  arrange(LokalitetID)
moran.test(moran$avg.agree,
           nb2listw(nb_katie, style="W",
                    zero.policy = TRUE),
           zero.policy = TRUE,
           na.action = na.omit)


```

```{r}
library(DHARMa)
library(glmmTMB)
# first we need to create a numeric factor recording the coordinates of the sampled locations
# 
dat <- data
dat$camCoords <- numFactor(scale(dat$Longitude), scale(dat$Latitude))
# then create a dummy group factor to be used as a random term
dat$ID <- factor(rep(1, nrow(dat)))

#Model using site location
## same as with lme4, but now we are using glmmTMB
# do we have to drop the fixed effect of cameras? It looks like you can, because you now account for grouping
# by coordinates, not by camera.
## mat stands for matern, which is a covariance function
# can read about all the covariance structures here: https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html
# mod2 <- glmmTMB(abs(agreement) ~ scale(Latitude) + scale(NDVImodis) + scale(treecanopycover) + binary.labels +
#                    mat(camCoords + 0 | ID), data = dat, family = gaussian()) ## try a different family? poisson
# summary(mod2)
# sims2 <- simulateResiduals(mod2)
# plot(sims2)

# mod3 <- glmmTMB(abs(agreement) ~ scale(Latitude) + scale(NDVImodis) + scale(treecanopycover) + binary.labels +
#                    (1|LokalitetID), data = dataA1_non_null, family = compois(link = "log") ) ## try a different family? poisson
# sims3 <- simulateResiduals(mod3)
# plot(sims3)


# library(MuMIn)
# r.squaredGLMM(mod2)#, null

```

Accounting for temporalautocorrelation

We will explore creating three different ways to have a more informed temporal variable

1. year
2. julian day
3. year + julian day
4. study date (julian day + 365 or 730 depending on study years 2019 or 2020)


```{r}
##from DHARMA
# res = simulateResiduals(global_model)
# # aggregating residuals by time
# res = recalculateResiduals(res, group = data$year)
# testTemporalAutocorrelation(res, time = unique(data$year))
# 
# # testing only subgroup location 1, could do same with loc 2
# res = recalculateResiduals(res, sel = data$LokalitetID == 864)
# testTemporalAutocorrelation(res, time = unique(data$year))

#install.packages('timeDate')
library(timeDate)

data$year <- as.integer(format(data$dateFormatted, format="%Y"))
data$DOY <- as.integer(format(data$dateFormatted, format="%j"))
data$julianDate <- round(timeDate::julian(data$dateFormatted, origin = timeDate("1970-01-01"), 
    units = "days")) ## relative julian day 

## create a dictionary of all the dates + a unique study date ID. It is a julian date + 365 for year 2019 and + 730 for year 2020
dates2018 <- seq(as.Date("2018-01-01"), as.Date("2018-04-01"), by="days")
dates2019 <- seq(as.Date("2018-10-01"), as.Date("2019-04-01"), by="days")
dates2020 <- seq(as.Date("2019-10-01"), as.Date("2020-04-01`"), by="days")
keys <- c(dates2018, dates2019, dates2020)
# values <- c()
# for (i in 1:length(keys)){
#   dateValue <- keys[i]
#   if (dateValue < as.Date('2018-09-30')){values <- c(values, i)}  ## 2018 dates
#   else if (dateValue > as.Date('2019-09-30')){values<- c(values, i + 730)}  # 2020 dates
#   else {values <- c(values, i+365)}
# }
# 
# ## create vector of study 
# studyDate <- c()
# for (i in 1:length(data$dateFormatted)){
#   index <- which(keys == data$dateFormatted[i])
#   #if (identical(index, integer(0))){print(data$dateFormatted[i])}
#   studyDate <- c(studyDate, values[index])
# }
# #data$studyDate <- studyDate ## should be same relationship as relative julianDate

## to get a nested value of date between Oct-Mar (1 - 120 so 12/31 and Jan 1 are next to each other)

dayOfWinterSeason <- c()
for (i in 1:length(data$dateFormatted)){  #length(data$dateFormatted
  dateValue <- data$dateFormatted[i]
  if (as.Date(dateValue) < as.Date('2018-09-30')){
    julDate <- round(julian(dateValue, origin = timeDate("2017-10-01"), units = "days")) 
    dayOfWinterSeason <- c(dayOfWinterSeason, julDate)}  ## 2018 dates
  else if (as.Date(dateValue) > as.Date('2019-09-30')){
    julDate <- round(julian(dateValue, origin = timeDate("2019-10-01"), units = "days")) 
    dayOfWinterSeason <- c(dayOfWinterSeason, julDate)}  # 2020 dates
  else {
    julDate <- round(julian(dateValue, origin = timeDate("2018-10-01"), units = "days")) 
    dayOfWinterSeason <- c(dayOfWinterSeason, julDate)}
}
data$dayofWinterSeason <- as.factor(dayOfWinterSeason)

mod_year <- glmmTMB(abs(agreement) ~ year, data = data)
summary(mod_year)
acf(residuals(mod_year))

mod_julian  <- glmmTMB(abs(agreement) ~ julianDate, data = data)
summary(mod_julian)
acf(residuals(mod_julian))

mod_dayOfWinterSeason <- glmmTMB(abs(agreement) ~ dayofWinterSeason, data = data)
summary(mod_dayOfWinterSeason)
acf(residuals(mod_dayOfWinterSeason))

# mod_studyD  <- glmmTMB(abs(agreement) ~ studyDate, data = data)
# summary(mod_studyD)
# acf(residuals(mod_studyD))

### they kind of all show the same thing that values closer in time are more related than values further in time 

```


```{r}
data$year <- as.factor(data$year)
data$julianDate <- as.factor(data$julianDate) ## relative to 1970-01-01
data$dayofWinterSeason <- as.factor(data$dayofWinterSeason)

mod_nested <- glmmTMB(abs(agreement) ~ scale(Latitude) + scale(NDVImodis) + scale(treecanopycover) + binary.labels + (1|LokalitetID) +
                   (1|year/dayofWinterSeason), data = data, family = gaussian()) ## t
summary(mod_nested)
sims_mod_temp <- simulateResiduals(mod_nested)
plot(sims_mod_temp)

```
Using ar1

https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html 
  - describes why it should be time + 0 | stand (it has something to do with a z matrix)
https://github.com/glmmTMB/glmmTMB/issues/329 


```{r}
### using ar1
mod_temp <- glmmTMB(abs(agreement) ~ scale(Latitude) + scale(NDVImodis) + scale(treecanopycover) + binary.labels + (1|LokalitetID) +
                   ar1(julianDate - 1 | LokalitetID), data = data, family = gaussian()) ## try a different family? poisson
summary(mod_temp)
sims_mod_temp <- simulateResiduals(mod_temp)
plot(sims_mod_temp)
## AIC: 71753.6

mod_temp0 <- glmmTMB(abs(agreement) ~ scale(Latitude) + scale(NDVImodis) + scale(treecanopycover) + binary.labels + (1|LokalitetID) +
                   ar1(julianDate + 0| LokalitetID), data = data, family = gaussian()) ## try a different family? poisson
summary(mod_temp0)
sims_mod_temp0 <- simulateResiduals(mod_temp0)
plot(sims_mod_temp0)
## AIC: 71753.6

mod_temp <- glmmTMB(abs(agreement) ~ scale(Latitude) + scale(NDVImodis) + scale(treecanopycover) + binary.labels + (1|LokalitetID),
                    data = data, family = gaussian()) ## try a different family? poisson
summary(mod_temp)
sims_mod_temp <- simulateResiduals(mod_temp)
plot(sims_mod_temp)
## AIC: 73583.8


```
testing for dispersion with the ar1 argument 
```{r}

```

Utilizing Alexej approach:
1. First make a relative date variable
2. Then test out different correlation structures 


```{r}
data$yr <- as.integer(as.character(data$year))
data$relyear <- 365*(data$yr-2018)
library(plyr)
relday <- plyr::ddply(data, "yr", summarise, mindat= min(dateFormatted), maxdat= max(dateFormatted))
relday$timdif <- relday$maxdat - relday$mindat

data <- dplyr::left_join(data, relday, by = "yr")

data$reldat <- data$dateFormatted - data$mindat
data$reldat <- round(as.integer(data$reldat)/60/60/24, digits =1) ## putting it in days
data$timdif <- as.integer(data$timdif)
data$date1 <- data$reldat/data$timdif ## not really sure if I use this 


```
## comparing different temporal models 

```{r}
## from above
mod_temp_julian <- glmmTMB(abs(agreement) ~ scale(Latitude) + scale(NDVImodis) + scale(treecanopycover) + binary.labels + (1|LokalitetID) + ar1(julianDate + 0| LokalitetID), data = data, family = gaussian()) ## try a different family? poisson
summary(mod_temp_julian)
sims_mod_temp_julian <- simulateResiduals(mod_temp_julian)
plot(sims_mod_temp_julian)
## AIC: 71753.6

mod_temp_rel <- glmmTMB(abs(agreement) ~ scale(Latitude) + scale(NDVImodis) + scale(treecanopycover) + binary.labels + (1|LokalitetID) + ar1(factor(reldat) + 0| LokalitetID), data = data, family = gaussian()) ## try a different family? poisson
summary(mod_temp_rel)
sims_mod_temp_rel <- simulateResiduals(mod_temp_rel)
plot(sims_mod_temp_rel)

```

## other correlation structures from Alexej
https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/corClasses.html

```{r}

# ## ask Alexej if I need to include family ??
# data_model <- na.omit(data)
# ## https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/corClasses.html
# full <- nlme::lme(abs(agreement) ~ scale(Latitude) + scale(NDVImodis) + scale(treecanopycover) + binary.labels, correlation = corAR1(form = ~ reldat + relyear), data=data_model, random = ~ 1|LokalitetID)
# full <- nlme::lme(abs(agreement) ~ scale(Latitude) + scale(NDVImodis) + scale(treecanopycover) + binary.labels, correlation = corCAR1(form = ~ reldat + relyear), data=data_model, random = ~ 1|LokalitetID)
# 
# summary(full)

```


### cloud gap filled agreement

```{r}

CGFdata <- na.omit(CGFdata)

CGFdata['SnowCover'] <- replace(CGFdata['SnowCover'], CGFdata['SnowCover'] == 4, 100)
CGFdata['SnowCover'] <- replace(CGFdata['SnowCover'], CGFdata['SnowCover'] == 3, 75)
CGFdata['SnowCover'] <- replace(CGFdata['SnowCover'], CGFdata['SnowCover'] == 2, 50)
CGFdata['SnowCover'] <- replace(CGFdata['SnowCover'], CGFdata['SnowCover'] == 1, 25)
CGFdata['SnowCover'] <- replace(CGFdata['SnowCover'], CGFdata['SnowCover'] == 0, 0)
CGFdata['agreement'] <- 100 - abs((CGFdata[,'SnowCover']) - (CGFdata[,'CGF_NDSI']))


cloud_gap_agreement <- function(data){
  n <- length(data$agreement)
  xbar <- mean(data$agreement)
  s <- sd(data$agreement)
  margin <- qt(0.95,df=n-1)*s/sqrt(n)
  lowerinterval <- xbar - margin
  upperinterval <- xbar + margin
  print(n) 
  print(xbar)
  print(s)
  print(lowerinterval) 
  print(upperinterval)
}

cloud_gap_agreement(CGFdata)
#hist(CGFdata$Cloud_Persistence, breaks = 30)

hist_dataCP <- ggplot(CGFdata, aes(x=Cloud_Persistence)) + 
  geom_histogram(color="black", fill="white") + theme(text = element_text(size=14), panel.background = element_blank(),
        axis.line = element_line(colour = "grey"),
        axis.text.x = element_text(angle = 0, hjust = 0.5, vjust = 1), legend.position = "none") + 
  labs(x = "Cloud Persistence", y = "Count", tag = "B")

hist_A1F_CP <- ggplot(MOD10A1F, aes(x=Cloud_Persistence)) + 
  geom_histogram(color="black", fill="white") + theme(text = element_text(size=14), panel.background = element_blank(),
        axis.line = element_line(colour = "grey"),
        axis.text.x = element_text(angle = 0, hjust = 0.5, vjust = 1), legend.position = "none") + 
  labs(x = "Cloud Persistence", y = "Count", tag = "A")
  
hist_dataCP
hist_A1F_CP
  

```


## gam for cloud cover

```{r}

dat_katie_CGFdata <- CGFdata
## add Lat/Long


#dat_katie <- dataA1_non_null 
stations_katieCGF <- data.frame(unique(dat_katie_CGFdata$LokalitetID))
names(stations_katieCGF)[names(stations_katieCGF)=="unique.dat_katie_CGFdata.LokalitetID."] <- "LokalitetID"

# gis_katieCGF <- data.frame(unique(dat_katie_CGFdata[,c("LokalitetID", "Latitude", "Longitude")]))
# gis_katieCGF <- as.matrix(gis_katieCGF[,2:3])
# 
# nb_katieCGF <- dnearneigh(gis_katieCGF,0,0.2) # 0.01 dec. deg. = 1.11 km
# 
# dat_katie_CGFdata <- dat_katie_CGFdata %>%
#   group_by(LokalitetID) %>%
#   mutate(avg.agree = mean(agreement))
# moranCGF <- dat_katie_CGFdata %>%
#   dplyr::select(LokalitetID, avg.agree)
# moranCGF <- unique(moran)
# moranCGF <- left_join(stations_katieCGF, moranCGF)
# moranCGF <- moranCGF %>%
#   arrange(LokalitetID)
# moran.test(moranCGF$avg.agree,
#            nb2listw(nb_katieCGF, style="W",
#                     zero.policy = TRUE),
#            zero.policy = TRUE,
#            na.action = na.omit)


cloud_persistence_analysis <- function(data){
  gam_CP <- mgcv::gam(agreement ~ s(Cloud_Persistence, k=8), data=data)
  #lmCP <- lm(agreement ~ Cloud_Persistence + I(Cloud_Persistence^2), data=data)
  summary(gam_CP); plot(gam_CP) #, pages = 1)
  print(summary(gam_CP))
  # visualise predictions from best model
  newdat_CP <- data.frame(Cloud_Persistence = seq(0,14, by = 0.5), agreement = 50)
  # 
  
  # then predict from best model to new dataframe
  predictionCGF_CP <- predict(gam_CP, newdat_CP, se.fit = T)
  newdat_CP <- newdat_CP %>%
    mutate(prediction = predictionCGF_CP[[1]],  se = predictionCGF_CP[[2]]) %>%
    mutate(lower_ci = prediction - se*1.96,
      upper_ci = prediction + se*1.96)

  modeledDataCGF_CP <- ggplot(newdat_CP, aes(Cloud_Persistence, prediction)) +
    theme_minimal() +
    geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.5, colour = NA) +
    geom_line(size = 1.5) +
    labs(x = "model-predicted satellite CGF values") 
  
  CP_lineOfBestFit <- ggplot(data=data, aes(x=Cloud_Persistence,y= agreement)) +
  theme(text = element_text(size=14), panel.background = element_blank(),
        axis.line = element_line(colour = "grey"),
        axis.text.x = element_text(angle = 0, hjust = 0.5, vjust = 1), legend.position = "none") +
  geom_point(size=0.5) + 
  geom_jitter(alpha = 0.3, size= 0.5)+
  geom_line(data=newdat_CP, aes(Cloud_Persistence, prediction), color='blue',size=2)+
  geom_ribbon(data =newdat_CP, aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.5, fill='blue')+
  labs(x = "Cloud persistence (days)", y = "Agreement (%)")+ scale_x_continuous(expand = c(0, 0), limits = c(0,14)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) + geom_smooth(method = "loess")

  return(CP_lineOfBestFit)
}

cloud_persistence_analysis(CGFdata)


```

## REVIEWER ADDITION
Fitting 2nd, 3rd, and 4th polynomial model instead of the GAM for the cloud persistence model

```{r}

cloud_persistence_POLY_analysis <- function(data, poly_degree){
  poly_CP <-lm(agreement ~ poly(Cloud_Persistence, degree = poly_degree), data = data)
  #lmCP <- lm(agreement ~ Cloud_Persistence + I(Cloud_Persistence^2), data=data)
  summary(poly_CP); plot(poly_CP) #, pages = 1)
  
  # visualise predictions from best model
  newdat_CP <- data.frame(Cloud_Persistence = seq(0,14, by = 0.5), agreement = 50)
  # 
  # then predict from best model to new dataframe
  predictionCGF_CP <- predict(poly_CP, newdat_CP, se.fit = T)
  newdat_CP <- newdat_CP %>%
    mutate(prediction = predictionCGF_CP[[1]],  se = predictionCGF_CP[[2]]) %>%
    mutate(lower_ci = prediction - se*1.96,
      upper_ci = prediction + se*1.96)

  modeledDataCGF_CP <- ggplot(newdat_CP, aes(Cloud_Persistence, prediction)) +
    theme_minimal() +
    geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.5, colour = NA) +
    geom_line(size = 1.5) +
    labs(x = "model-predicted satellite CGF values") 
  
  CP_lineOfBestFit <- ggplot(data=data, aes(x=Cloud_Persistence,y= agreement)) +
  theme(text = element_text(size=14), panel.background = element_blank(),
        axis.line = element_line(colour = "grey"),
        axis.text.x = element_text(angle = 0, hjust = 0.5, vjust = 1), legend.position = "none") +
  geom_point(size=0.5) + 
  geom_jitter(alpha = 0.3, size= 0.5)+
  geom_line(data=newdat_CP, aes(Cloud_Persistence, prediction), color='blue',size=2)+
  geom_ribbon(data =newdat_CP, aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.5, fill='blue')+
  labs(x = "Cloud persistence (days)", y = "Agreement (%)")+ scale_x_continuous(expand = c(0, 0), limits = c(0,14)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) + geom_smooth(method = "loess")
  print(newdat_CP)
  return(CP_lineOfBestFit)
}

cloud_persistence_POLY_analysis(CGFdata, 2)
cloud_persistence_POLY_analysis(CGFdata, 3)
cloud_persistence_POLY_analysis(CGFdata, 4)


```



###ROC analysis

SENSITIVITY: True positive rate
SPECIFICITY: True negative rate --> 1- SPECIFICITY is the False Positive Rate

```{r}
# what is type "resp"
# roc function takes the actual outcomes and the predicted values from the fit model

ROC_analysis <- function(data) {
  data <- data
  data$SnowCoverBinomial <-
    replace(data$SnowCover, data$SnowCover >= 1 , 1)
  
  snow1 <- sapply(data["SnowCoverBinomial"], function(x)
    x)
  
  score <- sapply(data["NDSImodis"], function(x)
    x)
  
  score <-
    abind::adrop(
      score,
      drop = 2,
      named.vector = TRUE,
      one.d.array = FALSE
    )
  
  
  logit_model <- glm(snow1 ~ score, family = "binomial")
  
  predictions <-
    stats::predict(logit_model, data.frame(score = score), type = "resp")
  
  roc <- pROC::roc(snow1, predictions) ## this isn't running?? ***
  
  # Can extract the thresholds used and associated sensitivities and specificities
  roc_data <- data.frame(
    threshold_probability = roc$thresholds,
    sens = roc$sensitivities,
    spec = roc$specificities
  )
  
  # Roc gives thresholds in terms of estimated probability according to the logistic model
  # from which we can back out the score threshold
  roc_data$threshold_log_odds <-
    with(roc_data, log(threshold_probability / (1 - threshold_probability)))
  roc_data$threshold_score <-
    (roc_data$threshold_log_odds - logit_model$coefficients[1]) / logit_model$coefficients[2]
  
  y <- roc_data$sens
  x <- 1 - roc_data$spec
  #plot(x,y)
  #to see a summary of sensitivity and specificity for all possible cut points: roc_data
  
  roc_data[10,] ## can do this up to 250 or so, depending on how many rows you want to see
  print(roc_data[1:90,])
  
  ## find top left corner
  # print('coordinates')
  # print(coords(roc, "best", ret = "threshold")) ## why is this 0.52
  # x_max <-  roc$thresholds[which.max(roc$sensitivities + roc$specificities)]
  # print('x_max')
  # print(x_max)
  # y_max <- roc$thresholds[which.max(roc$sensitivities + roc$specificities)]
  # print('y_max')
  # print(y_max)
  # print('roc_data')
  # roc_data
  # 
  # x_max1 <- roc$specificities[which(roc$thresholds == 40.5)]
  # print('x_max1')
  # print(x_max1)

    #rocPanel <- grid.arrange(rocPlot, p, nrow = 1)
  
  # use this one
  rocdataWide <- roc_data[, c(2, 3, 5)]
  print('rocdataWide')
  print(rocdataWide)
  rocdataWide$TrueNeg <- 1 - roc_data$spec
  rocdataWide$Youden <- roc_data$sens + roc_data$spec - 1
  YoudenIndex <- (max(rocdataWide$Youden))
  score_value <-
    rocdataWide[rocdataWide$Youden == YoudenIndex, "threshold_score"]
  print(score_value)
  x_max <- rocdataWide$spec[which(rocdataWide$threshold_score == score_value)]
  print('x_max')
  print(x_max)
  print('1-x_max')
  print(1-x_max)
  y_max <- rocdataWide$sens[which(rocdataWide$threshold_score == score_value)]
  print('y_max')
  print(y_max)
  
  
  ## 0.5260591 ## match to the right (below) in the roc table
  ## 0.52605908 0.9272606677 0.8607509        0.104330845       43.499379
  ## threshold is 43.499379
  
  ## visualize ROC plot and threshold Plot
  
  rocPlot <-
    ggplot(roc_data, aes(x = 1 - spec, y = sens), ) + geom_line(size = 1.5, color = "grey") +
    xlab("False Positive Rate") +
    ylab("True Positive Rate") +
    theme(
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      panel.background = element_blank(),
      text = element_text(size = 18),
      axis.line = element_line(colour = "black"),
    ) +
    scale_y_continuous(expand = c(0, 0), limits = c(0, 1.00)) +
    scale_x_continuous(expand = c(0, 0), limits = c(0, 1.05)) +
    geom_point(aes(x = 1 - x_max, y = y_max),
               colour = "blue",
               size = 4)
  
  print(rocPlot)
  
  rocdataWide <- rocdataWide[, c(1, 3, 4, 5)]
  rocdataLong <- reshape::melt(
    data = rocdataWide,
    id.vars = 'threshold_score',
    variable = "ROCOutput",
    value = "Value"
  )
  
  thresholdPlot <-
    ggplot(rocdataLong,
           aes(x = threshold_score , y = value, color = ROCOutput)) +
    geom_line(size = 1.5) +
    theme(
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      panel.background = element_blank(),
      text = element_text(size = 18),
      axis.line = element_line(colour = "black"),
      legend.position = c(0.78, 0.80),
      legend.title.align = 0.5,
      legend.title = element_text(size=12),
      legend.text = element_text(size=12)
    ) +
    xlab('MOD10A1 NDSI Snow Cover') + ylab('Rate') +
    scale_y_continuous(
      expand = c(0, 0),
      limits = c(0, 1.01),
      sec.axis = sec_axis(trans = ~ . * 1,  name = "Youden's Index")
    ) +
    scale_x_continuous(expand = c(0, 0), limits = c(0, 100)) +
    geom_vline(
      xintercept = score_value,
      linetype = "longdash",
      alpha = 0.9,
      colour = "blue"
    ) +
    geom_point(aes(x = score_value, y = (y_max - (1 - x_max))), colour =  #0.8878449794	0.8894723	0.62875996
                 "blue", size = 4) +
    scale_color_manual(
      name = "ROC Output",
      values = c("red", "orange", "green"),
      labels = c("True Positive Rate", "True Negative Rate", "Youden's Index"),
    )
  
  print(thresholdPlot)
  rocPanel <- grid.arrange(rocPlot, thresholdPlot, nrow = 1)
  return(rocPanel)
}


ROC_analysis(data) ## results show that 
#ROC_analysis(dat)

```

As requested by the reviewer, we will do the analysis for both open and canopy cover. 
This could be helpful depending on where you are doing your analysis, such as potentially above tree-line, or in agricultural areas.

So far all data is: 
[1] "rocdataWide"
[1] 40.49696
[1] "x_max"
[1] 0.8817326
[1] "1-x_max"
[1] 0.1182674
[1] "y_max"
[1] 0.8833522

closed_cc: 
[1] "rocdataWide"
[1] 40.49696
[1] "x_max"
[1] 0.8817326
[1] "1-x_max"
[1] 0.1182674
[1] "y_max"
[1] 0.8833522

open_cc: 
[1] "rocdataWide"
[1] 41.49575
[1] "x_max"
[1] 0.9115827
[1] "1-x_max"
[1] 0.08841733
[1] "y_max"
[1] 0.8947368

```{r}
data_closed_cc <- data[data$treecanopycover > 20,]
data_open_cc <- data[data$treecanopycover < 20,]

ROC_analysis(data_closed_cc)
ROC_analysis(data_open_cc) 

```


NDVImax for supplemental 

```{r}
##NDVImax
## filter on year
NDVImaxFunction <- function(data){
  data1 <- dplyr::filter(data, (dateFormatted >= as.POSIXct('2017-10-01') & dateFormatted <= as.POSIXct('2018-04-01')))
  data1$NDVImax <- rep(mean(data1$NDVImodis), length(data1$NDVImodis))
  data2 <- dplyr::filter(data, (dateFormatted >= as.POSIXct('2018-10-01') & dateFormatted <= as.POSIXct('2019-04-01')))
  data2$NDVImax <- rep(mean(data2$NDVImodis), length(data2$NDVImodis))
  data3 <- dplyr::filter(data, (dateFormatted >= as.POSIXct('2019-10-01') & dateFormatted <= as.POSIXct('2020-04-01')))
  data3$NDVImax <- rep(mean(data3$NDVImodis), length(data3$NDVImodis))
  data4 <- rbind(data1, data2, data3)
  return(data4)
  }

NDVImaxdf <- NDVImaxFunction(dataA1_non_null)

global_model_wNDVImax <- lmerTest::lmer(abs(agreement) ~
                          (1|LokalitetID) +
                          scale(Latitude) + 
                         scale(NDVImax) + scale(treecanopycover) + binary.labels, data = NDVImaxdf) ### what family 
global_model_wNDVImax
summary(global_model_wNDVImax)
```

## end

Comparing results to Luo paper by looking at if there cameras were at steeper locations than ours

```{r}

slope <- read.csv('/Users/catherinebreen/Documents/Chapter 3/data/slope.csv')
slope$LokalitetID <- factor(slope$LokalitetID)
all_cameras <- left_join(dat, slope)
all_cameras <- all_cameras[!is.na(all_cameras$mean),]
all_cameras <- all_cameras[unique(all_cameras$LokalitetID),]
hist(all_cameras$mean)

avg.slope <- mean(all_cameras$mean)
min.slope <- min(all_cameras$mean)
max.slope <- max(all_cameras$mean)

ffc <- read.csv('/Users/catherinebreen/Documents/RemoteSensingPaper/datasets/forest_fractional_cover_forest_only.csv')
ffc$LokalitetID <- factor(ffc$LokalitetID)
all_cameras_ffc <- left_join(dat, ffc)
all_cameras_ffc <- all_cameras_ffc[!is.na(all_cameras_ffc$mean),]
all_cameras_ffc <- all_cameras_ffc[unique(all_cameras_ffc$LokalitetID),]
hist(all_cameras_ffc$mean)

```

d
